{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "Y-PrOm4WwDRd",
        "SGXAG-FwwLj0",
        "STnsuQR8Hr6w"
      ],
      "authorship_tag": "ABX9TyMH5xQNr9XjDEnxYb1GRamQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kchenTTP/python-series/blob/main/error_handling_and_exceptions_in_python/Error_Handling_and_Testing_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Error Handling and Testing in Python**\n",
        "\n",
        "This is the second class in a two-part series on Python errors, how they occur, and how to handle them effectively.\n",
        "\n",
        "Previously, we explored different types of errors in Python and how to check for various data types ([see last class](https://colab.research.google.com/drive/1zSHSUZFXAgBJjqKpJv41GmjRfmlf7KlO?usp=sharing)). In this session, we'll focus on handling errors properly and introduce unit testing to help ensure our code runs reliably.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Table of Contents**\n",
        "\n",
        "- [Handling Errors & Exceptions](#scrollTo=2-spc3mG9BiP)\n",
        "- [Testing Your Code](#scrollTo=22qEVIyKG_i0)\n"
      ],
      "metadata": {
        "id": "2zqFUEgCwaSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Handling Errors & Exceptions**\n",
        "\n",
        "In Python, you can handle exceptions using `try`, `except`, `else`, and `finally` blocks to manage different outcomes in your code:\n",
        "\n",
        "- **`try`**: Use this block to wrap code that might raise an exception. If an error occurs, Python will jump to the `except` block.\n",
        "- **`except`**: This block handles the error, allowing the program to continue running or to provide a helpful error message instead of crashing. Multiple `except` blocks are allowed.\n",
        "- **`else`**: If no exceptions occur in the `try` block, the code in the `else` block will run. This is useful for code that should only execute when no errors are raised.\n",
        "- **`finally`**: This block will always run, regardless of whether an exception was raised or not. It's typically used for cleanup tasks, like closing files or releasing resources.\n"
      ],
      "metadata": {
        "id": "2-spc3mG9BiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Handling Generic Exceptions**\n",
        "\n",
        "If you are unsure what kind of exception might occur, you can use a generic `except` block to handle any exception. However, it is generally recommended to avoid catching all exceptions this way unless absolutely necessary, as it can mask important errors and make debugging more challenging.\n",
        "\n",
        "> üí° **Important:** When possible, specify the exceptions you want to catch explicitly instead of using a generic catch all exception for better error management.\n"
      ],
      "metadata": {
        "id": "hlY90HYLWRZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generic Exceptions\n",
        "try:\n",
        "  print(nonexistent_var)\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  print(\"...do something else\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZZmXVH49prm",
        "outputId": "24816207-4d4e-4e5e-8728-248bd53ae834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name 'nonexistent_var' is not defined\n",
            "...do something else\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Handling Specific Exceptions**\n",
        "\n",
        "Using specific exceptions (like `ValueError` or `ZeroDivisionError`) improves code clarity, as it's clearer which errors you expect and handle.\n",
        "\n",
        "Generic exceptions should be reserved for cases where you genuinely need to capture any unexpected issue, such as logging errors in critical applications or unknown input conditions.\n",
        "\n",
        "> *Uncomment each print statement individually to see how Python handles specific exceptions.*\n"
      ],
      "metadata": {
        "id": "ZFmVHqTgWpQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "\n",
        "dict1 = {\"1\": \"apple\", \"2\": \"banana\"}\n",
        "l1 = [1, 2, 3, 4]\n",
        "\n",
        "try:\n",
        "  # Uncomment one line at a time to see each exception handling in action\n",
        "\n",
        "  # Name error\n",
        "  # print(nonexistent_var)\n",
        "\n",
        "  # Key error\n",
        "  # print(dict1[\"3\"])\n",
        "\n",
        "  # HTTPError\n",
        "  resp = requests.get(\"https://api.spoonacular.com/recipes/complexSearch?query=pasta&maxFat=25&number=2\")\n",
        "  resp.raise_for_status()  # Raises HTTPError for unsuccessful status codes\n",
        "\n",
        "  # Index error (other kinds of unspecified error)\n",
        "  # print(l1[5])\n",
        "except NameError as e:\n",
        "  print(\"This is a NameError: A variable is being used before it is defined.\")\n",
        "except KeyError as e:\n",
        "  print(\"This is a KeyError: Trying to access a dictionary key that doesn‚Äôt exist.\")\n",
        "except requests.exceptions.HTTPError as e:\n",
        "  print(\"This is an HTTPError: A request was unsuccessful (e.g., bad status code).\")\n",
        "except Exception as e:\n",
        "  print(\"This is a generic exception, which can be used to catch all unspecified exceptions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZw9wpmz-wZK",
        "outputId": "05ca0411-c717-4ee1-b006-65a2d9b84641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an HTTPError: A request was unsuccessful (e.g., bad status code).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ‚ùó‚ùó It is important to note that when you handle specific exceptions, any exceptions that aren't explicitly listed will still cause an error and stop the program.\n"
      ],
      "metadata": {
        "id": "IjLKk1ZlwkQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  print(nonexistent_var)\n",
        "except TypeError as _:\n",
        "  print(\"No TypeError in try block, failed to catch error\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "g_WAbXZK-H8g",
        "outputId": "84ea75cf-2cde-442e-f3ef-1a06d0a1a552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nonexistent_var' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2b287ee1d36b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnonexistent_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No TypeError in try block, failed to catch error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nonexistent_var' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l2 = [\"red\", \"green\", \"blue\", \"orange\"]\n",
        "print(l2)\n",
        "idx = input(\"Input the index of a color you would like to see: \")\n",
        "\n",
        "try:\n",
        "  print(l2[int(idx)])\n",
        "except IndexError as e:\n",
        "  print(f\"Index Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "CYwFx9PHFNys",
        "outputId": "7baf99ce-14a2-4d20-c69f-d5aecd633bb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['red', 'green', 'blue', 'orange']\n",
            "Input the index of a color you would like to see: blue\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "invalid literal for int() with base 10: 'blue'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-29e05a6010c1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Index Error: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'blue'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, in the code above, I initially only checked for an `IndexError`, assuming it would be the only potential error. However, by running the code and observing all exceptions, I discovered a `ValueError` that could also occur when the program tries to convert a non-numeric string to an integer.\n",
        "\n",
        "> üìí **NOTE:** *Catching these unexpected errors also help improves my understanding of my own code and helps me better understand how it behaves in various scenarios.*\n"
      ],
      "metadata": {
        "id": "FmSaAvIKG576"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Handling Cases with No Exceptions and Cleanup**\n",
        "\n",
        "The `else` and `finally` blocks can be used to handle situations where no exceptions occur and to perform any necessary cleanup, regardless of whether an exception was raised.\n",
        "\n",
        "- **`else`**: Executes only if no exceptions were raised in the `try` block. This is useful for code that should only run when everything goes smoothly.\n",
        "- **`finally`**: Always executes, whether an exception was raised or not. This block is ideal for cleanup tasks, such as closing files or releasing resources.\n",
        "\n",
        "<br>\n",
        "\n",
        "Let's look at an example:\n"
      ],
      "metadata": {
        "id": "GgG9TpAd_00A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  file = open(\"example.txt\", \"r\")\n",
        "  content = file.read()\n",
        "except FileNotFoundError:\n",
        "  print(\"Error: The file was not found.\")\n",
        "else:\n",
        "  print(\"File read successfully!\")\n",
        "  print(content)\n",
        "finally:\n",
        "  if 'file' in locals():\n",
        "    file.close()\n",
        "    print(\"File closed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_oIufzlzpsG",
        "outputId": "4e698818-5a9d-4c86-9c9d-e1ac782a3ff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The file was not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "\n",
        "1. **`try`**: Attempts to open and read from `example.txt`.\n",
        "2. **`except`**: Catches a `FileNotFoundError` if the file doesn't exist, preventing a crash.\n",
        "3. **`else`**: Runs only if the file is found and read successfully, allowing you to print the file's content.\n",
        "4. **`finally`**: Ensures the file is closed whether or not an exception occurred to prevent potential resource leaks.\n"
      ],
      "metadata": {
        "id": "ywiD9T0Uz32l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Example: Average Function**\n",
        "\n",
        "Now create a function called `average` that average numbers to practice what we've learned so far. Make sure you accomodate for every kind of error this function might encounter.\n",
        "\n",
        "> üí° *HINT: On top of using the `try` and `except` block, you can also use `isinstance()` which we've learned in our last class to check for datatypes*\n",
        "\n",
        "> *PS. Your function can take a list, a tuple, or multiple arguments*\n"
      ],
      "metadata": {
        "id": "mERQAgj4IM_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n"
      ],
      "metadata": {
        "id": "bt5Jav0hstq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Solution 1**"
      ],
      "metadata": {
        "id": "Y-PrOm4WwDRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def average(num_list: list | tuple) -> float | None:\n",
        "  \"\"\"A function that takes a list of numbers and return the average\"\"\"\n",
        "\n",
        "  if not isinstance(num_list, (list, tuple)):\n",
        "    raise TypeError(\"Input must be a list or tuple of numbers\")\n",
        "\n",
        "  if not all(isinstance(num, (int, float)) for num in num_list):\n",
        "    raise TypeError(\"List must contain only numbers\")\n",
        "\n",
        "  try:\n",
        "    return sum(num_list) / len(num_list)\n",
        "  except ZeroDivisionError:\n",
        "    print(\"Cannot calculate average of an empty list\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "uKflqIpIspY0"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your code\n",
        "avg = average([1, 2, 3, 4, 5])\n",
        "print(avg)"
      ],
      "metadata": {
        "id": "vMVEEx0RuH32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test our try except block\n",
        "avg = average([])\n",
        "print(avg)"
      ],
      "metadata": {
        "id": "rZkZwSA0uZtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test errors\n",
        "avg = average(\"apples\")\n",
        "print(avg)"
      ],
      "metadata": {
        "id": "-UVLXK4DuUk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test errors\n",
        "avg = average(0)\n",
        "print(avg)"
      ],
      "metadata": {
        "id": "1xVY4RKovHv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test errors\n",
        "avg = average([\"20\", \"banana\", 5.6])\n",
        "print(avg)"
      ],
      "metadata": {
        "id": "R4SbEFXRuyl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Solution 2**"
      ],
      "metadata": {
        "id": "SGXAG-FwwLj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def average2(*numbers: int | float) -> float | None:\n",
        "  \"\"\"A function that takes an arbitrary amount of numbers as arguments and returns the average\"\"\"\n",
        "\n",
        "  if not all(isinstance(num, (int, float)) for num in numbers):\n",
        "    raise TypeError(\"Argument must contain only numbers\")\n",
        "\n",
        "  try:\n",
        "    return sum(numbers) / len(numbers)\n",
        "  except ZeroDivisionError:\n",
        "    print(\"Function requires at least 1 numerical argument\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "1OQM0t7LwMII"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your code\n",
        "avg = average2(1, 2, 3, 4, 5)\n",
        "print(avg)"
      ],
      "metadata": {
        "id": "KN8eIA5lwdqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test our try except block\n",
        "avg = average2()\n",
        "print(avg)"
      ],
      "metadata": {
        "id": "VS9FValJwn3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test errors\n",
        "avg = average2(\"apples\")\n",
        "print(avg)"
      ],
      "metadata": {
        "id": "Xe-Cy_15wr0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your code\n",
        "avg = average2(0)\n",
        "print(avg)"
      ],
      "metadata": {
        "id": "hyoi5j_zw8Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test errors\n",
        "avg = average2(\"20\", \"banana\", 5.6)\n",
        "print(avg)"
      ],
      "metadata": {
        "id": "SONsC2uUxFBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing Your Code**\n",
        "\n",
        "Testing is an essential part of software development to ensure your code works as expected and handles errors properly.\n",
        "\n",
        "While there are various approaches and methodologies for testing, such as **unit tests**, **integration tests**, and **system tests**. We'll be focusing on the basics of **unit testing**, which involves testing individual functions or components in isolation.\n"
      ],
      "metadata": {
        "id": "22qEVIyKG_i0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Getting Started with Unit Testing**\n",
        "\n",
        "To start unit testing, create test cases for specific functions or components to verify that they work as expected.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Benefits of unit testing**\n",
        "\n",
        "- **Verify functionality**: Confirm that each part of your code behaves as expected.\n",
        "- **Catch bugs early**: Identify issues during development, reducing problems later on.\n",
        "- **Improve maintainability**: Simplify future code changes by confirming that core behaviors remain stable.\n",
        "\n",
        "Let's take a look at how you can test your code with unit testing. If your code is incorrect, then your test cases should catch the errors:\n"
      ],
      "metadata": {
        "id": "03D9NL7sGPWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the function to be tested\n",
        "def minus(x: int | float, y: int | float) -> int | float:\n",
        "  return x - y"
      ],
      "metadata": {
        "id": "U1V68XHEJCpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write some test cases to make sure the results from the function is correct\n",
        "\n",
        "res = minus(2, 3)\n",
        "assert res == -1\n",
        "\n",
        "res = minus(-1, 1)\n",
        "assert res == -2\n",
        "\n",
        "res = minus(0, 0)\n",
        "assert res == 0\n",
        "\n",
        "print(\"3 test passed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChH-EVCvKk00",
        "outputId": "61bef694-9536-40d3-aa6e-cf28428cff73"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 test passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is implemented correctly, nothing will happen. However, if there is error in your code, then your test cases should catch them.\n",
        "\n",
        "You can even make your unit test handle multiple different testing parameters, which is also known as **parameterized testing** like this example below:\n"
      ],
      "metadata": {
        "id": "-xK9AaaXK-Vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Error in function\n",
        "def square(x: int | float) -> int | float:\n",
        "  return x ** 2  # incorrect operation"
      ],
      "metadata": {
        "id": "8m9rzxegLFS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameterized testing\n",
        "\n",
        "def test_square(test_cases: list):\n",
        "  if not isinstance(test_cases, list):\n",
        "    raise TypeError(\"Input must be a list\")\n",
        "\n",
        "  error_count = 0\n",
        "  errors: list[tuple] = []\n",
        "\n",
        "  for tc in test_cases:\n",
        "    try:\n",
        "      res = square(tc)\n",
        "      assert res == tc ** 2, f\"Function {square.__name__}: Expected result {tc ** 2}, got {res} instead\"\n",
        "      print(f\"Test case {tc} passed\")\n",
        "    except AssertionError as e:\n",
        "      error_count += 1\n",
        "      errors.append((tc, e))\n",
        "    except Exception as e:\n",
        "      error_count += 1\n",
        "      errors.append((tc, e))\n",
        "\n",
        "  print()\n",
        "\n",
        "  if error_count == 0:\n",
        "    print(\"All test cases passed\")\n",
        "  else:\n",
        "    print(f\"{error_count} out of {len(test_cases)} test cases failed\")\n",
        "    print(\"==========\")\n",
        "    for tc, error in errors:\n",
        "      print(f\"Input: {tc}\")\n",
        "      print(f\"Error: {error}\")\n",
        "      print()"
      ],
      "metadata": {
        "id": "B2nkWMABLXWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_square([2, -1, \"apple\", 3j + 2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOfdsFuQ45X7",
        "outputId": "1fdeea20-5a3e-4059-b434-2d17347cc911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test case 2 passed\n",
            "Test case -1 passed\n",
            "Test case (2+3j) passed\n",
            "\n",
            "1 out of 4 test cases failed\n",
            "==========\n",
            "Input: apple\n",
            "Error: unsupported operand type(s) for ** or pow(): 'str' and 'int'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Example: Testing Our Average Function**\n",
        "\n",
        "Now let us write some unit tests for our [`average()` function](#scrollTo=Y-PrOm4WwDRd) we just created earlier. You can write a simple test that test one test case at a time or use parameterized testing to test for multiple test cases.\n",
        "\n",
        "> üí° *HINT: Use `assert` statements to test our function*\n",
        "\n",
        "> *PS. Make sure your testing functions start with \"test_<func_name>\" as it is the convention for unit tests*\n"
      ],
      "metadata": {
        "id": "L3hI_HXK0Ddd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n"
      ],
      "metadata": {
        "id": "sXdT6_by0ECY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Simple Tests Solution\n",
        "\n",
        "def test_average_1():\n",
        "    assert average([1, 2, 3, 4, 5]) == 3\n",
        "\n",
        "def test_average_2():\n",
        "    assert average([]) is None\n",
        "\n",
        "def test_average_3():\n",
        "  try:\n",
        "    assert average(\"apples\") is None\n",
        "  except TypeError as e:\n",
        "    if str(e) == \"Input must be a list or tuple of numbers\":\n",
        "      pass\n",
        "    else:\n",
        "      raise e\n",
        "\n",
        "def test_average_4():\n",
        "  try:\n",
        "    assert average(0) is None\n",
        "  except TypeError as e:\n",
        "    if str(e) == \"Input must be a list or tuple of numbers\":\n",
        "      pass\n",
        "    else:\n",
        "      raise e\n",
        "\n",
        "def test_average_5():\n",
        "  try:\n",
        "    assert average([\"20\", \"banana\", 5.6]) is None\n",
        "  except TypeError as e:\n",
        "    if str(e) == \"List must contain only numbers\":\n",
        "      pass\n",
        "    else:\n",
        "      raise e\n",
        "\n",
        "# Run tests\n",
        "test_average_1()\n",
        "test_average_2()\n",
        "test_average_3()\n",
        "test_average_4()\n",
        "test_average_5()\n",
        "print(\"All test cases passed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "2OCcUIPW-Lgz",
        "outputId": "3737696c-fd57-4875-912d-afa28f536216"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cannot calculate average of an empty list\n",
            "All test cases passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parameterized Testing Solution\n",
        "\n",
        "def test_average(test_cases: list):\n",
        "  if not isinstance(test_cases, list):\n",
        "    raise TypeError(\"Input must be a list\")\n",
        "\n",
        "  error_count = 0\n",
        "  errors: list[tuple] = []\n",
        "\n",
        "  for tc in test_cases:\n",
        "    try:\n",
        "      res = average(tc)\n",
        "      sol = sum(tc) / len(tc)\n",
        "      assert res == sol , f\"Function {average.__name__}: Expected result {sol}, got {res} instead\"\n",
        "      print(f\"Test case {tc} passed\")\n",
        "    except AssertionError as e:\n",
        "      error_count += 1\n",
        "      errors.append((tc, e))\n",
        "    except ZeroDivisionError as e:\n",
        "      if 'res' in locals() and res is None:  # Check if res exists and is None\n",
        "        print(f\"Test case {tc} passed\")\n",
        "      else:\n",
        "        error_count += 1\n",
        "        errors.append((tc, e))\n",
        "    except Exception as e:\n",
        "      if str(e) in (\n",
        "          \"List must contain only numbers\",\n",
        "          \"Input must be a list or tuple of numbers\",\n",
        "        ):\n",
        "        print(f\"Test case {tc} passed\")\n",
        "        continue\n",
        "      else:\n",
        "        error_count += 1\n",
        "        errors.append((tc, e))\n",
        "\n",
        "  print()\n",
        "\n",
        "  if error_count == 0:\n",
        "    print(\"All test cases passed\")\n",
        "  else:\n",
        "    print(f\"{error_count} out of {len(test_cases)} test cases failed\")\n",
        "    print(\"==========\")\n",
        "    for tc, error in errors:\n",
        "      print(f\"Input: {tc}\")\n",
        "      print(f\"Error: {error}\")\n",
        "      print()"
      ],
      "metadata": {
        "id": "OEuvm7cD8trX",
        "cellView": "form"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameterized testing\n",
        "test_cases = [\n",
        "    [1, 2, 3, 4, 5],\n",
        "    \"apples\",\n",
        "    0,\n",
        "    [\"20\", \"banana\", 5.6],\n",
        "    [],\n",
        "]\n",
        "test_average(test_cases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxSEpbym9Juw",
        "outputId": "2f7a5851-be90-4586-d967-c6e8564f75ad"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test case [1, 2, 3, 4, 5] passed\n",
            "Test case apples passed\n",
            "Test case 0 passed\n",
            "Test case ['20', 'banana', 5.6] passed\n",
            "Cannot calculate average of an empty list\n",
            "Test case [] passed\n",
            "\n",
            "All test cases passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This concludes our section about testing.\n",
        "\n",
        "As we can see, writing your own test is quite a lot of work. Which is why a majority of developers will leverage testing libraries such as [unittest](https://docs.python.org/3/library/unittest.html) or [pytest](https://docs.pytest.org/en/stable/).\n",
        "\n",
        "We won't be covering those libraries in this class but feel free to read the documentation or the next section to learn more about them.\n"
      ],
      "metadata": {
        "id": "hNlddxYNAJh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[pytest](https://docs.pytest.org/en/stable/) (Additional Material)**\n",
        "\n",
        "> üìí **NOTE:** This section covers a testing library that is not very suitable for Colab or Jupyter notebooks. Feel free to skip this section if you mainly use a notebook style environment. You can still write test functions to test your code like what we did above.\n"
      ],
      "metadata": {
        "id": "STnsuQR8Hr6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`pytest` is a popular testing framework in Python that simplifies writing and running tests. It automatically discovers and runs test cases, provides clear error messages, and offers useful testing features for both beginners and experienced developers.\n",
        "\n",
        "> üö® `pytest` expects `.py` files for test scripts, which can be a bit tricky to use directly in Google Colab. To work around this, you can write test functions within a cell, save it as a file using `%%file <filename>` in the first line of the cell, and use `!pytest` commands in Colab, or run `pytest` locally after saving test scripts as `.py` files.\n"
      ],
      "metadata": {
        "id": "_t3W3x9G0NBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**With `pytest`, you can:**\n",
        "\n",
        "- Write simple test functions to check specific outputs or behaviors.\n",
        "- Easily run tests with informative reports.\n",
        "- Use features like exception testing and test coverage, which make debugging easier and faster.\n",
        "\n",
        "<br>\n",
        "\n",
        "Let's see how we can use `pytest` to streamline unit testing:\n",
        "\n",
        "<br>\n",
        "\n",
        "First, we want to create a file that contains all our functions:\n",
        "\n",
        "> *The first line save this cell to a file called `my_module.py` on the disk when executed. This is a Colab magic command.*\n",
        "\n"
      ],
      "metadata": {
        "id": "D8aUoCMAJOXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file my_module.py\n",
        "\"\"\"\n",
        "This is the file where we write all our functions.\n",
        "\"\"\"\n",
        "\n",
        "def add(a: int | float, b: int | float) -> int | float:\n",
        "  return a + b\n",
        "\n",
        "def plus_one(x: int | float) -> int | float:\n",
        "  return x + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLBnJe1IJe2l",
        "outputId": "fd58dcff-7121-4fdc-8e77-9cf7b0491f7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing my_module.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, create a file that contains all our test cases:\n",
        "\n",
        "> *To write a unit test, create test functions named `test_<functionality>`*\n"
      ],
      "metadata": {
        "id": "1Xv_GbBYWCKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file test_my_module.py\n",
        "\"\"\"\n",
        "This is the file where we write all our tests.\n",
        "\"\"\"\n",
        "import pytest\n",
        "from my_module import add, plus_one  # Import functions from our `my_module.py` file\n",
        "\n",
        "\n",
        "def test_add():\n",
        "  assert add(2, 3) == 5\n",
        "  assert add(-1, 1) == 0\n",
        "\n",
        "def test_plus_one():\n",
        "  assert plus_one(2) == 3\n",
        "  assert plus_one(-1) == 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96IOk5OZVQw8",
        "outputId": "83f21760-a160-490b-d0ce-dd26ef2ed94a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_my_module.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, to run the tests, simply use `pytest` in the terminal:\n",
        "\n",
        "```sh\n",
        "pytest <test_filename>\n",
        "```\n",
        "\n",
        "> *We can use `!` in Colab to execute shell commands. Just make sure `pytest` is installed*\n"
      ],
      "metadata": {
        "id": "AN0h_M49WNRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest test_my_module.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ4Pdq4IWmNd",
        "outputId": "21ae44a8-80d3-43ff-a3f3-16bfe9d27aff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.11.11, pytest-8.3.5, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: typeguard-4.4.2, langsmith-0.3.13, anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 2 items                                                                                  \u001b[0m\n",
            "\n",
            "test_my_module.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                         [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Parametrizing Tests**\n",
        "\n",
        "`pytest` allows you to easily **parametrize** test functions, enabling you to run the same test with multiple inputs and expected outputs without writing multiple `assert` statements. This helps keep your code cleaner and allows you to quickly test multiple cases, including expected exceptions.\n"
      ],
      "metadata": {
        "id": "7Y7bMU8qYdm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file test_file_parametrize.py\n",
        "\"\"\"\n",
        "Updated test cases with parameters.\n",
        "\"\"\"\n",
        "import pytest\n",
        "from my_module import add, plus_one\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize(\"x\", [1, 2, 3])\n",
        "@pytest.mark.parametrize(\"y\", [-1, 0, 2])\n",
        "def test_add(x, y):\n",
        "  assert add(x, y) == x + y\n",
        "\n",
        "@pytest.mark.parametrize(\"x\", [-1, 0, 1, 2, 3.5])\n",
        "def test_plus_one(x):\n",
        "  assert plus_one(x) == x + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcNrOMTlZd7i",
        "outputId": "3ee4b46f-9f0e-43e1-9b28-13ca18e4377a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_file_parametrize.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest test_file_parametrize.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1PWTBnYaHSO",
        "outputId": "048a373e-3961-46ad-ef49-871f5234df5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.11.11, pytest-8.3.5, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: typeguard-4.4.2, langsmith-0.3.13, anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 14 items                                                                                 \u001b[0m\n",
            "\n",
            "test_file_parametrize.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                      [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m14 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing Exceptions with `pytest`**\n",
        "\n",
        "When testing functions that may raise exceptions, you can use the `pytest.raises` context manager to ensure specific errors are raised when expected.\n"
      ],
      "metadata": {
        "id": "rBOeqjXDHsPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file div_module.py\n",
        "\"\"\"\n",
        "This is the file containing our divide function.\n",
        "\"\"\"\n",
        "\n",
        "def divide(a: int | float, b: int | float) -> int | float:\n",
        "  if b == 0:\n",
        "    raise ValueError(\"Cannot divide by zero\")\n",
        "  return a / b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmQdxQhG-T-i",
        "outputId": "6dcabe2a-47e2-4ed6-c59a-a0452fb03386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing div_module.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file test_div_module.py\n",
        "\"\"\"\n",
        "This is the file where we test the divide function.\n",
        "\"\"\"\n",
        "import pytest\n",
        "from div_module import divide\n",
        "\n",
        "\n",
        "def test_divide_by_zero():\n",
        "  with pytest.raises(ValueError, match=\"Cannot divide by zero\"):\n",
        "    divide(10, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CELnGx8k-uDM",
        "outputId": "3dc1a468-f76c-4780-a5aa-4d047d05bc2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_div_module.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest test_div_module.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EucNGs8j_F76",
        "outputId": "b8914747-ca88-4405-8afd-51934436dbe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.11.11, pytest-8.3.5, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: typeguard-4.4.2, langsmith-0.3.13, anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 1 item                                                                                   \u001b[0m\n",
            "\n",
            "test_div_module.py \u001b[32m.\u001b[0m\u001b[32m                                                                         [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Checking Test Coverage**\n",
        "\n",
        "To ensure you've tested all important parts of your code, you can check **test coverage** using the `pytest-cov` plugin. This shows the percentage of your code covered by tests, helping you identify any gaps.\n"
      ],
      "metadata": {
        "id": "hVabpcE5-UQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install\n",
        "!pip install pytest-cov"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy2twLzz_cYF",
        "outputId": "241e7914-295a-427e-f493-ff75448a486f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytest-cov\n",
            "  Downloading pytest_cov-6.0.0-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: pytest>=4.6 in /usr/local/lib/python3.11/dist-packages (from pytest-cov) (8.3.5)\n",
            "Collecting coverage>=7.5 (from coverage[toml]>=7.5->pytest-cov)\n",
            "  Downloading coverage-7.6.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest>=4.6->pytest-cov) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest>=4.6->pytest-cov) (24.2)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=4.6->pytest-cov) (1.5.0)\n",
            "Downloading pytest_cov-6.0.0-py3-none-any.whl (22 kB)\n",
            "Downloading coverage-7.6.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m241.0/241.0 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: coverage, pytest-cov\n",
            "Successfully installed coverage-7.6.12 pytest-cov-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run `pytest` with coverage:\n",
        "\n",
        "```sh\n",
        "pytest --cov=<module_filename> <test_filename>\n",
        "```\n",
        "\n",
        "> *Do not include `.py` file extension for the module file you are testing*\n"
      ],
      "metadata": {
        "id": "3qyuflkZAHJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest --cov=my_module test_my_module.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gBqewh1_hKM",
        "outputId": "517cc2ee-63a7-4e57-ab32-2520e520e59d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.11.11, pytest-8.3.5, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: cov-6.0.0, typeguard-4.4.2, langsmith-0.3.13, anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 2 items                                                                                  \u001b[0m\n",
            "\n",
            "test_my_module.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                         [100%]\u001b[0m\n",
            "\n",
            "---------- coverage: platform linux, python 3.11.11-final-0 ----------\n",
            "Name           Stmts   Miss  Cover\n",
            "----------------------------------\n",
            "my_module.py       4      0   100%\n",
            "----------------------------------\n",
            "TOTAL              4      0   100%\n",
            "\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.06s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**\n",
        "\n",
        "That wraps up our mini-series on Errors and Exception Handling in Python! Understanding how to properly handle errors and write tests is crucial for writing reliable code.\n",
        "\n",
        "For further reading, check out these resources:\n",
        "- [Unit Testing in Python - DataQuest](https://www.dataquest.io/blog/unit-tests-python/)\n",
        "- [Unit Testing with `unittest` - GeeksforGeeks](https://www.geeksforgeeks.org/unit-testing-python-unittest/)\n",
        "- [Getting Started with `pytest`](https://docs.pytest.org/en/stable/getting-started.html)\n",
        "\n",
        "<br>\n",
        "\n",
        "Keep practicing, and don't forget that handling errors and writing tests will save you time debugging in the long run. Happy coding! üêç\n"
      ],
      "metadata": {
        "id": "0FWnYbrQA6M1"
      }
    }
  ]
}