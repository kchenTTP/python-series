{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "STnsuQR8Hr6w"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPDeD83L0hiuQJBVcL5ySUa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kchenTTP/python-series/blob/main/error_handling_and_exceptions_in_python/Error_Handling_and_Testing_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Error Handling and Testing in Python**\n",
        "\n",
        "This is the second class in a two-part series on Python errors, how they occur, and how to handle them effectively.\n",
        "\n",
        "Previously, we explored different types of errors in Python and how to check for various data types to minimize errors ([see last class](https://colab.research.google.com/drive/1zSHSUZFXAgBJjqKpJv41GmjRfmlf7KlO?usp=sharing)). In this session, we'll focus on handling errors during runtime and introduce unit testing to help ensure our code runs reliably.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Table of Contents**\n",
        "\n",
        "- [Handling Errors & Exceptions](#scrollTo=2-spc3mG9BiP)\n",
        "- [Testing Your Code](#scrollTo=22qEVIyKG_i0)\n"
      ],
      "metadata": {
        "id": "2zqFUEgCwaSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Handling Errors & Exceptions**\n",
        "\n",
        "In Python, when error occurs, your program stop and report back to you the error it encountered:\n"
      ],
      "metadata": {
        "id": "2-spc3mG9BiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read some data\n",
        "with open(\"nonexistent_file.txt\", \"r\") as f:\n",
        "  data = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "ZNXR2w17JxG7",
        "outputId": "dde799c7-71fb-4ea3-d8cc-2eacf447b6bd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'nonexistent_file.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7eee064b3c66>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read some data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nonexistent_file.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'nonexistent_file.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, \"letting it crash\" might not always be the right solution. Sometimes you have to let Python know how you want to handle the exception after it occurs so that your program can continue to run.\n"
      ],
      "metadata": {
        "id": "OEOx9vsXJXp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, you can handle exceptions using `try`, `except`, `else`, and `finally` blocks to manage different outcomes in your code:\n",
        "\n",
        "- **`try`**: Use this block to wrap code that might raise an exception. If an error occurs, Python will jump to the `except` block.\n",
        "- **`except`**: This block handles the error, allowing the program to continue running or to provide a helpful error message instead of crashing. Multiple `except` blocks are allowed.\n",
        "- **`else`**: If no exceptions occur in the `try` block, the code in the `else` block will run. This is useful for code that should only execute when no errors are raised.\n",
        "- **`finally`**: This block will always run, regardless of whether an exception was raised or not. It's typically used for cleanup tasks, like closing files or releasing resources.\n"
      ],
      "metadata": {
        "id": "eXQ9xKs-KaQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Handling Generic Exceptions**\n",
        "\n",
        "If you are unsure what kind of exception might occur, you can use a generic `except` block to handle any exception. However, it is generally recommended to avoid catching all exceptions this way unless absolutely necessary, as it can mask important errors and make debugging more challenging.\n",
        "\n",
        "> üí° **Important:** When possible, specify the exceptions you want to catch explicitly instead of using a generic catch all exception for better error management.\n"
      ],
      "metadata": {
        "id": "hlY90HYLWRZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generic Exceptions\n",
        "try:\n",
        "  print(nonexistent_var)\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  print(\"...do something else\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZZmXVH49prm",
        "outputId": "cb7b311e-60df-4550-8ab4-688ed4335f5b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name 'nonexistent_var' is not defined\n",
            "...do something else\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Handling Specific Exceptions**\n",
        "\n",
        "Using specific exceptions (like `ValueError` or `ZeroDivisionError`) improves code clarity, as it's clearer which errors you expect and handle.\n",
        "\n",
        "Generic exceptions should be reserved for cases where you genuinely need to capture any unexpected issue, such as logging errors in critical applications or unknown input conditions.\n",
        "\n",
        "> *Uncomment each print statement individually to see how Python handles specific exceptions.*\n"
      ],
      "metadata": {
        "id": "ZFmVHqTgWpQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "\n",
        "dict1 = {\"1\": \"apple\", \"2\": \"banana\"}\n",
        "l1 = [1, 2, 3, 4]\n",
        "\n",
        "try:\n",
        "  # Uncomment one line at a time to see each exception handling in action\n",
        "\n",
        "  # Name error\n",
        "  # print(nonexistent_var)\n",
        "\n",
        "  # Key error\n",
        "  # print(dict1[\"3\"])\n",
        "\n",
        "  # HTTPError\n",
        "  resp = requests.get(\"https://api.spoonacular.com/recipes/complexSearch?query=pasta&maxFat=25&number=2\")\n",
        "  resp.raise_for_status()  # Raises HTTPError for unsuccessful status codes\n",
        "\n",
        "  # Index error (other kinds of unspecified error)\n",
        "  # print(l1[5])\n",
        "except NameError as e:\n",
        "  print(\"This is a NameError: A variable is being used before it is defined.\")\n",
        "except KeyError as e:\n",
        "  print(\"This is a KeyError: Trying to access a dictionary key that doesn‚Äôt exist.\")\n",
        "except requests.exceptions.HTTPError as e:\n",
        "  print(\"This is an HTTPError: A request was unsuccessful (e.g., bad status code).\")\n",
        "except Exception as e:\n",
        "  print(\"This is a generic exception, which can be used to catch all unspecified exceptions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZw9wpmz-wZK",
        "outputId": "a838c32c-779b-4426-9cfc-f9f3f8107d73"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an HTTPError: A request was unsuccessful (e.g., bad status code).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ‚ùó‚ùó It is important to note that when you handle specific exceptions, any exceptions that aren't explicitly listed will still cause an error and stop the program.\n"
      ],
      "metadata": {
        "id": "IjLKk1ZlwkQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  print(nonexistent_var)\n",
        "except TypeError as _:\n",
        "  print(\"No TypeError in try block, failed to catch error\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "g_WAbXZK-H8g",
        "outputId": "2b877623-2473-4918-d7ad-0b7c089ced92"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nonexistent_var' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2b287ee1d36b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnonexistent_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No TypeError in try block, failed to catch error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nonexistent_var' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l2 = [\"red\", \"green\", \"blue\", \"orange\"]\n",
        "print(l2)\n",
        "idx = input(\"Input the index of a color you would like to see: \")\n",
        "\n",
        "try:\n",
        "  print(l2[int(idx)])\n",
        "except IndexError as e:\n",
        "  print(f\"Index Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "CYwFx9PHFNys",
        "outputId": "c2fea021-ed6f-4871-fce8-0be01360c1f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['red', 'green', 'blue', 'orange']\n",
            "Input the index of a color you would like to see: blue\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "invalid literal for int() with base 10: 'blue'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-29e05a6010c1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Index Error: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'blue'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, in the code above, we initially checked only for an `IndexError`, assuming it was the only possible issue. However, by running the code and observing all exceptions, we discovered that a `ValueError` could also occur when the program attempts to convert a non-numeric string to an integer.\n",
        "\n",
        "> üìí **NOTE:** *Catching unexpected errors is crucial in software development, as it improves your understanding of your code and helps you see how it behaves in different scenarios.*\n"
      ],
      "metadata": {
        "id": "FmSaAvIKG576"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Handling Cases with No Exceptions and Cleanup**\n",
        "\n",
        "The `else` and `finally` blocks can be used to handle situations where no exceptions occur and to perform any necessary cleanup, regardless of whether an exception was raised.\n",
        "\n",
        "- **`else`**: Executes only if no exceptions were raised in the `try` block. This is useful for code that should only run when everything goes smoothly.\n",
        "- **`finally`**: Always executes, whether an exception was raised or not. This block is ideal for cleanup tasks, such as closing files or releasing resources.\n",
        "\n",
        "<br>\n",
        "\n",
        "Let's look at an example:\n"
      ],
      "metadata": {
        "id": "GgG9TpAd_00A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  file = open(\"./sample_data/README.md\", \"r\")\n",
        "  content = file.readline()\n",
        "except FileNotFoundError:\n",
        "  print(\"Error: The file was not found.\")\n",
        "else:\n",
        "  print(\"File read successfully!\")\n",
        "  print(content)\n",
        "finally:\n",
        "  if 'file' in locals():\n",
        "    file.close()\n",
        "    print(\"File closed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_oIufzlzpsG",
        "outputId": "43e0ed7d-a157-481a-c569-e8a4e73db234"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File read successfully!\n",
            "This directory includes a few sample datasets to get you started.\n",
            "\n",
            "File closed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "\n",
        "1. **`try`**: Attempts to open and read from `./sample_data/README.md`.\n",
        "2. **`except`**: Catches a `FileNotFoundError` if the file doesn't exist, preventing a crash.\n",
        "3. **`else`**: Runs only if the file is found and read successfully, allowing you to print the first line of the file.\n",
        "4. **`finally`**: Ensures the file is closed whether or not an exception occurred to prevent potential resource leaks.\n"
      ],
      "metadata": {
        "id": "ywiD9T0Uz32l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Example: Average Function**\n",
        "\n",
        "Now create a function called `average` that average numbers to practice what we've learned so far. Make sure you accomodate for every kind of error this function might encounter.\n",
        "\n",
        "> üí° *HINT: On top of using the `try` and `except` block, you can also use `isinstance()` which we've learned in our last class to check for datatypes*\n",
        "\n",
        "> *PS. Your function can take a list, a tuple, or multiple arguments*\n"
      ],
      "metadata": {
        "id": "mERQAgj4IM_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n"
      ],
      "metadata": {
        "id": "bt5Jav0hstq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #### **Solution 1: List Input**\n",
        "\n",
        "def average1(num_list: list | tuple) -> float | None:\n",
        "  \"\"\"A function that takes a list of numbers and return the average\"\"\"\n",
        "\n",
        "  if not isinstance(num_list, (list, tuple)):\n",
        "    print(\"Input must be a list or tuple of numbers\")\n",
        "    return None\n",
        "\n",
        "  if not all(isinstance(num, (int, float)) for num in num_list):\n",
        "    print(\"List must contain only numbers\")\n",
        "    return None\n",
        "\n",
        "  try:\n",
        "    return sum(num_list) / len(num_list)\n",
        "  except ZeroDivisionError:\n",
        "    print(\"Cannot calculate average of an empty list\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "uKflqIpIspY0",
        "cellView": "form"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #### **Solution 2: Variable Length Input**\n",
        "\n",
        "def average2(*numbers: int | float) -> float | None:\n",
        "  \"\"\"A function that takes an arbitrary amount of numbers as arguments and returns the average\"\"\"\n",
        "\n",
        "  if not all(isinstance(num, (int, float)) for num in numbers):\n",
        "    print(\"Argument must contain only numbers\")\n",
        "    return None\n",
        "\n",
        "  try:\n",
        "    return sum(numbers) / len(numbers)\n",
        "  except ZeroDivisionError:\n",
        "    print(\"Function requires at least 1 numerical argument\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "1OQM0t7LwMII",
        "cellView": "form"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing Your Code**\n",
        "\n",
        "Testing is an essential part of software development to ensure your code works as expected and handles errors properly.\n",
        "\n",
        "While there are various approaches and paradigms for testing, such as **unit tests**, **integration tests**, and **system tests**. We'll be focusing on the basics of **unit testing**, which involves testing individual functions or components in isolation.\n"
      ],
      "metadata": {
        "id": "22qEVIyKG_i0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Getting Started with Unit Testing**\n",
        "\n",
        "To start unit testing, create test cases for specific functions or components and use `assert` to verify that they give you the expected result.\n"
      ],
      "metadata": {
        "id": "03D9NL7sGPWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`assert`**\n",
        "\n",
        "The **`assert`** statement raises an `AssertionError` if the given condition is *False*. It is especially useful for debugging and writing simple tests to verify that your code behaves as expected. You can also provide an optional string that will show up when the assertion fails.\n"
      ],
      "metadata": {
        "id": "9ixwI-uF2dUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert True\n",
        "assert False, \"Message for failed assertion\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "1Zl8ry0_Rtqj",
        "outputId": "ecf4546b-5ba4-4968-fece-b126647baa53"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Message for failed assertion",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-1f639fde10c8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Message for failed assertion\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: Message for failed assertion"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's write some tests for the `average` function we just created:\n"
      ],
      "metadata": {
        "id": "9_7hii15Rt_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write some test cases to make sure the results from the function is correct\n",
        "\n",
        "avg = average1([1, 2, 3, 4, 5])\n",
        "assert avg == 3\n",
        "\n",
        "avg = average1([0, 0, 0, 0, 0])\n",
        "assert avg == 0\n",
        "\n",
        "avg = average1([])\n",
        "assert avg == None\n",
        "\n",
        "avg = average1(\"apples\")\n",
        "assert avg == None\n",
        "\n",
        "avg = average1(0)\n",
        "assert avg == None\n",
        "\n",
        "avg = average1([\"20\", \"banana\", 5.6])\n",
        "assert avg == None\n",
        "\n",
        "print(\"6 test passed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChH-EVCvKk00",
        "outputId": "72d42f94-2662-4c84-8382-9a3632cd51d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cannot calculate average of an empty list\n",
            "Input must be a list or tuple of numbers\n",
            "Input must be a list or tuple of numbers\n",
            "List must contain only numbers\n",
            "6 test passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefits of unit testing**\n",
        "\n",
        "- **Verify functionality**: Confirm that each part of your code behaves as expected.\n",
        "- **Catch bugs early**: Identify issues during development, reducing problems later on.\n",
        "- **Improve maintainability**: Simplify future code changes by confirming that core behaviors remain stable.\n"
      ],
      "metadata": {
        "id": "-xK9AaaXK-Vd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Example: Unit Tests**\n",
        "\n",
        "Now let's create some tests for this `update_dictionary` function which either updates a value associated with a key in a dictionary or deletes a key-value pair.\n",
        "\n",
        "**Our tests should verify**\n",
        "\n",
        "1. the function behaves as expected for both update and delete operations\n",
        "2. when attempting to delete a non-existent key or when an invalid mode is provided, the function will either return the original dictionary or raise a ValueError.\n",
        "\n",
        "> üí° *PS. Make sure your testing functions start with \"test_<func_name>\" as it is the convention for unit tests*\n"
      ],
      "metadata": {
        "id": "L3hI_HXK0Ddd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Literal\n",
        "\n",
        "def update_dictionary(my_dict: dict, mode: Literal[\"u\", \"d\"], key: str, value: str | None = None) -> dict:\n",
        "  match mode:\n",
        "    case \"u\":\n",
        "      my_dict[key] = value\n",
        "    case \"d\":\n",
        "      try:\n",
        "        my_dict.pop(key)\n",
        "      except KeyError:\n",
        "        print(f\"Key '{key}' not found. No key deleted.\")\n",
        "        return my_dict\n",
        "    case _:\n",
        "      raise ValueError(\"Invalid mode. Please choose 'u' for update or 'd' for delete.\")\n",
        "  return my_dict"
      ],
      "metadata": {
        "id": "DKTdJ2gFZMdt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage\n",
        "my_dict = {\"apple\": 1, \"banana\": 2, \"cherry\": 3}\n",
        "print(update_dictionary(my_dict, \"u\", \"banana\", 5))\n",
        "print(update_dictionary(my_dict, \"u\", \"apple\", 10))\n",
        "print(update_dictionary(my_dict, \"d\", \"cherry\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrU6HS0IuHq0",
        "outputId": "e7da486d-b13b-46ad-b9a5-50a51b029d9a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'apple': 1, 'banana': 5, 'cherry': 3}\n",
            "{'apple': 10, 'banana': 5, 'cherry': 3}\n",
            "{'apple': 10, 'banana': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "def test_update_existing_key():\n",
        "  ...\n",
        "\n",
        "def test_update_new_key():\n",
        "  ...\n",
        "\n",
        "def test_delete_existing_key():\n",
        "  ...\n",
        "\n",
        "def test_delete_nonexistent_key():\n",
        "  ...\n",
        "\n",
        "def test_invalid_mode():\n",
        "  ..."
      ],
      "metadata": {
        "id": "sXdT6_by0ECY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "def test_update_existing_key():\n",
        "  init_dict = {\"a\": 1, \"b\": 2}\n",
        "  updated_dict = update_dictionary(init_dict.copy(), \"u\", \"b\", 3)\n",
        "  assert updated_dict == {\"a\": 1, \"b\": 3}\n",
        "  print(\"test_update_existing_key passed\")\n",
        "\n",
        "def test_update_new_key():\n",
        "  init_dict = {\"a\": 1, \"b\": 2}\n",
        "  updated_dict = update_dictionary(init_dict.copy(), \"u\", \"c\", 4)\n",
        "  assert updated_dict == {\"a\": 1, \"b\": 2, \"c\": 4}\n",
        "  print(\"test_update_new_key passed\")\n",
        "\n",
        "def test_delete_existing_key():\n",
        "  init_dict = {\"a\": 1, \"b\": 2}\n",
        "  updated_dict = update_dictionary(init_dict.copy(), \"d\", \"b\")\n",
        "  assert updated_dict == {\"a\": 1}\n",
        "  print(\"test_delete_existing_key passed\")\n",
        "\n",
        "def test_delete_nonexistent_key():\n",
        "  init_dict = {\"a\": 1, \"b\": 2}\n",
        "  updated_dict = update_dictionary(init_dict.copy(), \"d\", \"c\")\n",
        "  assert updated_dict == {\"a\": 1, \"b\": 2}\n",
        "  print(\"test_delete_nonexistent_key passed\")\n",
        "\n",
        "def test_invalid_mode():\n",
        "  init_dict = {\"a\": 1, \"b\": 2}\n",
        "  try:\n",
        "    update_dictionary(init_dict, \"x\", \"a\", 5)\n",
        "  except ValueError as e:\n",
        "    assert str(e) == \"Invalid mode. Please choose 'u' for update or 'd' for delete.\"\n",
        "    print(\"test_invalid_mode passed\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2OCcUIPW-Lgz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_update_existing_key()\n",
        "test_update_new_key()\n",
        "test_delete_existing_key()\n",
        "test_delete_nonexistent_key()\n",
        "test_invalid_mode()"
      ],
      "metadata": {
        "id": "nDntCtEGvj_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, writing your own test is quite a lot of work. Which is why a majority of developers will leverage testing libraries such as [unittest](https://docs.python.org/3/library/unittest.html) or [pytest](https://docs.pytest.org/en/stable/).\n"
      ],
      "metadata": {
        "id": "hNlddxYNAJh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[pytest](https://docs.pytest.org/en/stable/) (Additional Material)**\n",
        "\n",
        "> üìí **NOTE:** This section covers a testing library that is not very suitable for Colab or Jupyter notebooks. Feel free to skip this section if you mainly use a notebook style environment. You can still write test functions to test your code like what we did above.\n"
      ],
      "metadata": {
        "id": "STnsuQR8Hr6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`pytest` is a popular testing framework in Python that simplifies writing and running tests. It automatically discovers and runs test cases, provides clear error messages, and offers useful testing features for both beginners and experienced developers.\n",
        "\n",
        "> üö® `pytest` expects `.py` files for test scripts, which can be a bit tricky to use directly in Google Colab. To work around this, you can write test functions within a cell, save it as a file using `%%file <filename>` in the first line of the cell, and use `!pytest` commands in Colab, or run `pytest` locally after saving test scripts as `.py` files.\n"
      ],
      "metadata": {
        "id": "_t3W3x9G0NBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**With `pytest`, you can:**\n",
        "\n",
        "- Write simple test functions to check specific outputs or behaviors.\n",
        "- Easily run tests with informative reports.\n",
        "- Use features like exception testing and test coverage, which make debugging easier and faster.\n",
        "\n",
        "<br>\n",
        "\n",
        "First, we want to create a file that contains all our functions:\n",
        "\n",
        "> *The first line save this cell to a file called `my_module.py` on the disk when executed. This is a Colab magic command.*\n",
        "\n"
      ],
      "metadata": {
        "id": "D8aUoCMAJOXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file my_module.py\n",
        "\"\"\"\n",
        "This is the file where we write all our functions.\n",
        "\"\"\"\n",
        "\n",
        "def add(a: int | float, b: int | float) -> int | float:\n",
        "  return a + b\n",
        "\n",
        "def plus_one(x: int | float) -> int | float:\n",
        "  return x + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLBnJe1IJe2l",
        "outputId": "fd58dcff-7121-4fdc-8e77-9cf7b0491f7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing my_module.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, create a file that contains all our test cases:\n",
        "\n",
        "> *To write a unit test, create test functions named `test_<functionality>`*\n"
      ],
      "metadata": {
        "id": "1Xv_GbBYWCKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file test_my_module.py\n",
        "\"\"\"\n",
        "This is the file where we write all our tests.\n",
        "\"\"\"\n",
        "import pytest\n",
        "from my_module import add, plus_one  # Import functions from our `my_module.py` file\n",
        "\n",
        "\n",
        "def test_add():\n",
        "  assert add(2, 3) == 5\n",
        "  assert add(-1, 1) == 0\n",
        "\n",
        "def test_plus_one():\n",
        "  assert plus_one(2) == 3\n",
        "  assert plus_one(-1) == 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96IOk5OZVQw8",
        "outputId": "83f21760-a160-490b-d0ce-dd26ef2ed94a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_my_module.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, to run the tests, simply use `pytest` in the terminal:\n",
        "\n",
        "```sh\n",
        "pytest <test_filename>\n",
        "```\n",
        "\n",
        "> *We can use `!` in Colab to execute shell commands. Just make sure `pytest` is installed*\n"
      ],
      "metadata": {
        "id": "AN0h_M49WNRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest test_my_module.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ4Pdq4IWmNd",
        "outputId": "21ae44a8-80d3-43ff-a3f3-16bfe9d27aff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.11.11, pytest-8.3.5, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: typeguard-4.4.2, langsmith-0.3.13, anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 2 items                                                                                  \u001b[0m\n",
            "\n",
            "test_my_module.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                         [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Parametrizing Tests**\n",
        "\n",
        "`pytest` allows you to easily **parametrize** test functions, enabling you to run the same test with multiple inputs and expected outputs without writing multiple `assert` statements. This helps keep your code cleaner and allows you to quickly test multiple cases, including expected exceptions.\n"
      ],
      "metadata": {
        "id": "7Y7bMU8qYdm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file test_file_parametrize.py\n",
        "\"\"\"\n",
        "Updated test cases with parameters.\n",
        "\"\"\"\n",
        "import pytest\n",
        "from my_module import add, plus_one\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize(\"x\", [1, 2, 3])\n",
        "@pytest.mark.parametrize(\"y\", [-1, 0, 2])\n",
        "def test_add(x, y):\n",
        "  assert add(x, y) == x + y\n",
        "\n",
        "@pytest.mark.parametrize(\"x\", [-1, 0, 1, 2, 3.5])\n",
        "def test_plus_one(x):\n",
        "  assert plus_one(x) == x + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcNrOMTlZd7i",
        "outputId": "3ee4b46f-9f0e-43e1-9b28-13ca18e4377a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_file_parametrize.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest test_file_parametrize.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1PWTBnYaHSO",
        "outputId": "048a373e-3961-46ad-ef49-871f5234df5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.11.11, pytest-8.3.5, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: typeguard-4.4.2, langsmith-0.3.13, anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 14 items                                                                                 \u001b[0m\n",
            "\n",
            "test_file_parametrize.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                      [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m14 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing Exceptions with `pytest`**\n",
        "\n",
        "When testing functions that may raise exceptions, you can use the `pytest.raises` context manager to ensure specific errors are raised when expected.\n"
      ],
      "metadata": {
        "id": "rBOeqjXDHsPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file div_module.py\n",
        "\"\"\"\n",
        "This is the file containing our divide function.\n",
        "\"\"\"\n",
        "\n",
        "def divide(a: int | float, b: int | float) -> int | float:\n",
        "  if b == 0:\n",
        "    raise ValueError(\"Cannot divide by zero\")\n",
        "  return a / b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmQdxQhG-T-i",
        "outputId": "6dcabe2a-47e2-4ed6-c59a-a0452fb03386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing div_module.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file test_div_module.py\n",
        "\"\"\"\n",
        "This is the file where we test the divide function.\n",
        "\"\"\"\n",
        "import pytest\n",
        "from div_module import divide\n",
        "\n",
        "\n",
        "def test_divide_by_zero():\n",
        "  with pytest.raises(ValueError, match=\"Cannot divide by zero\"):\n",
        "    divide(10, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CELnGx8k-uDM",
        "outputId": "3dc1a468-f76c-4780-a5aa-4d047d05bc2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_div_module.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest test_div_module.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EucNGs8j_F76",
        "outputId": "b8914747-ca88-4405-8afd-51934436dbe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.11.11, pytest-8.3.5, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: typeguard-4.4.2, langsmith-0.3.13, anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 1 item                                                                                   \u001b[0m\n",
            "\n",
            "test_div_module.py \u001b[32m.\u001b[0m\u001b[32m                                                                         [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Checking Test Coverage**\n",
        "\n",
        "To ensure you've tested all important parts of your code, you can check **test coverage** using the `pytest-cov` plugin. This shows the percentage of your code covered by tests, helping you identify any gaps.\n"
      ],
      "metadata": {
        "id": "hVabpcE5-UQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install\n",
        "!pip install pytest-cov"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy2twLzz_cYF",
        "outputId": "241e7914-295a-427e-f493-ff75448a486f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytest-cov\n",
            "  Downloading pytest_cov-6.0.0-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: pytest>=4.6 in /usr/local/lib/python3.11/dist-packages (from pytest-cov) (8.3.5)\n",
            "Collecting coverage>=7.5 (from coverage[toml]>=7.5->pytest-cov)\n",
            "  Downloading coverage-7.6.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest>=4.6->pytest-cov) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest>=4.6->pytest-cov) (24.2)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=4.6->pytest-cov) (1.5.0)\n",
            "Downloading pytest_cov-6.0.0-py3-none-any.whl (22 kB)\n",
            "Downloading coverage-7.6.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m241.0/241.0 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: coverage, pytest-cov\n",
            "Successfully installed coverage-7.6.12 pytest-cov-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run `pytest` with coverage:\n",
        "\n",
        "```sh\n",
        "pytest --cov=<module_filename> <test_filename>\n",
        "```\n",
        "\n",
        "> *Do not include `.py` file extension for the module file you are testing*\n"
      ],
      "metadata": {
        "id": "3qyuflkZAHJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest --cov=my_module test_my_module.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gBqewh1_hKM",
        "outputId": "517cc2ee-63a7-4e57-ab32-2520e520e59d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.11.11, pytest-8.3.5, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: cov-6.0.0, typeguard-4.4.2, langsmith-0.3.13, anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 2 items                                                                                  \u001b[0m\n",
            "\n",
            "test_my_module.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                         [100%]\u001b[0m\n",
            "\n",
            "---------- coverage: platform linux, python 3.11.11-final-0 ----------\n",
            "Name           Stmts   Miss  Cover\n",
            "----------------------------------\n",
            "my_module.py       4      0   100%\n",
            "----------------------------------\n",
            "TOTAL              4      0   100%\n",
            "\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.06s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**\n",
        "\n",
        "That wraps up our mini-series on Errors and Exception Handling in Python! Understanding how to properly handle errors and write tests is crucial for writing reliable code.\n",
        "\n",
        "For further reading, check out these resources:\n",
        "- [Unit Testing in Python - DataQuest](https://www.dataquest.io/blog/unit-tests-python/)\n",
        "- [Unit Testing with `unittest` - GeeksforGeeks](https://www.geeksforgeeks.org/unit-testing-python-unittest/)\n",
        "- [Getting Started with `pytest`](https://docs.pytest.org/en/stable/getting-started.html)\n",
        "\n",
        "<br>\n",
        "\n",
        "Keep practicing, and don't forget that handling errors and writing tests will save you time debugging in the long run. Happy coding! üêç\n"
      ],
      "metadata": {
        "id": "0FWnYbrQA6M1"
      }
    }
  ]
}